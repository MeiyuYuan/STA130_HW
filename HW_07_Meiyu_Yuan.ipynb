{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21ff040c",
   "metadata": {},
   "source": [
    "1. Simple linear regression predicts an outcome using a single predictor variable, with a form like Y=β0+β1X. Multiple linear regression extends this by including multiple predictors, represented as Y=β0+β1X1+β2X2+...+βnXn. This approach captures more influencing factors and provides better insights and predictions.\n",
    "    Continuous variables in simple linear regression represent values that change smoothly, as in Y=β0+β1X. Indicator variables, on the other hand, use 0s and 1s to indicate group membership, such as Y=β0+β11(X). Continuous variables model trends, while indicator variables capture group differences.\n",
    "    Adding an indicator variable to a continuous variable in multiple regression yields a model like Y=β0+β1X+β21(Z). This allows for different intercepts for different groups while maintaining the continuous relationship with X.\n",
    "    Introducing an interaction term between a continuous variable and an indicator variable creates a model such as Y=β0+β1X+β21(Z)+β3(X×1(Z)). This means that the effect of X varies by group, giving different slopes for each group.\n",
    "    For categorical variables with kkk categories, using k-1 indicator variables captures all groups while avoiding redundancy. For example, with categories A, B, and C, indicators 1(B) and 1(C) are used, with A as the baseline. The model compares each category to this baseline and shows the differences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663702a0",
   "metadata": {},
   "source": [
    "2. To make predictions using these two formulas, the no-interaction model simply adds up the separate effects of TV and online advertising: Y=β0+β1X1+β2X2. This means that each type of advertising affects sales independently; increasing the TV budget has the same effect no matter what the online spending is. But with the interaction model, Y=β0+β1X1+β2X2+β3(X1×X2), it takes into account how the effectiveness of one ad type depends on the other. Here, an increase in TV budget could have a greater or different effect if online spending is high, showing a more complex relationship.\n",
    "    When we treat ad budgets as \"high\" or \"low\" (using binary variables), the no-interaction model predicts sales with Y=β0+β11(TV_High)+β21(Online_High). That is, high budgets for TV and online simply add their effects on sales. But with an interaction term, the formula becomes Y=β0+β11(TV_High)+β21(Online_High)+β3(1(TV_High)×1(Online_High)). Now, having both high budgets together can change sales differently than having just one, showing how the two ad types can work together.\n",
    "    \n",
    "link of chat log histories: https://chatgpt.com/share/6733ef70-8a7c-8004-96b3-161ba99926fc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b002a19",
   "metadata": {},
   "source": [
    "3. \n",
    "    (1) Fitting a Logistic Regression Model: This logistic regression model uses age, province, and an extraversion score as predictors to predict the binary outcome (gender as a binary indicator)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee00f984",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "data = pd.read_csv(\"C:\\\\Users\\\\skdto\\\\Downloads\\\\CSCS_data_anon.csv\")\n",
    "\n",
    "data['binary_outcome'] = (data['DEMO_gender'] == 'Woman').astype(int)  # Example binary indicator based on 'DEMO_gender'\n",
    "\n",
    "formula = 'binary_outcome ~ DEMO_age + C(GEO_province) + PSYCH_big_five_inventory_extraverted_score'\n",
    "log_reg = smf.logit(formula, data=data).fit()\n",
    "\n",
    "print(log_reg.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02e4ed4",
   "metadata": {},
   "source": [
    "    (2) Fitting an Additive Multiple Linear Regression Model: This model predicts a continuous outcome (extraversion score) using age, province, and an agreeableness score. The predictors are assumed to have independent (additive) effects on the outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc9512e",
   "metadata": {},
   "outputs": [],
   "source": [
    "formula_linear = 'PSYCH_big_five_inventory_extraverted_score ~ DEMO_age + C(GEO_province) + PSYCH_big_five_inventory_agreeable_score'\n",
    "lin_reg = smf.ols(formula_linear, data=data).fit()\n",
    "\n",
    "print(lin_reg.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d972f3b3",
   "metadata": {},
   "source": [
    "    (3) Adding Interaction Terms for Synergistic Effects: Adding interaction terms allows the effect of one predictor (age) to change based on another predictor (extraversion score). This captures any combined or synergistic effects between the predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf80117c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For logistic regression\n",
    "formula_interaction_logit = 'binary_outcome ~ DEMO_age * PSYCH_big_five_inventory_extraverted_score + C(GEO_province)'\n",
    "log_reg_interaction = smf.logit(formula_interaction_logit, data=data).fit()\n",
    "print(log_reg_interaction.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f64c005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For linear regression\n",
    "formula_interaction_linear = 'PSYCH_big_five_inventory_extraverted_score ~ DEMO_age * PSYCH_big_five_inventory_agreeable_score + C(GEO_province)'\n",
    "lin_reg_interaction = smf.ols(formula_interaction_linear, data=data).fit()\n",
    "print(lin_reg_interaction.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900102ac",
   "metadata": {},
   "source": [
    "    (4) Interpreting the models: \n",
    "    Without interaction: In this case, each predictor contributes to the outcome independently. The coefficients represent the average change in the outcome for a unit change in the predictor, assuming other variables are held constant.\n",
    "    With interaction: The interaction term allows the effect of one predictor on the outcome to vary with the value of another predictor, providing a more nuanced understanding of relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba6e324",
   "metadata": {},
   "source": [
    "    (5) Visualization: \n",
    "        For logistic regression: Though logistic regression works with probabilities (log-odds), you can interpret it as a linear model for simplicity and plot fitted values against predictors.\n",
    "        For linear regression: Use or to plot predicted values and lines illustrating the relationships between predictors and outcomes for both additive and interaction models.(matplotlib)(plotly)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122cb901",
   "metadata": {},
   "source": [
    "4. The apparent contradiction between \"the model only explains 17.6% of the variability in the data\" (low R^2) and \"many of the coefficients are large with strong evidence against the null hypothesis\" (small p-values) arises because these metrics assess different aspects of model performance. A small p-value (such as p<0.05) indicates strong evidence against the null hypothesis, suggesting that the predictor variable is likely to have a non-zero (and potentially substantial) effect on the outcome variable. R^2 measures the overall proportion of variability in the outcome explained by the entire model, and a low value indicates that much of the variability remains unexplained—possibly due to unmeasured factors or noise. In contrast, p-values assess whether individual predictors have statistically significant relationships with the outcome, holding other variables constant. So, even if the model as a whole doesn't explain much variability (low R^2), individual predictors can still have strong, significant effects (low p-values). This situation is common in complex systems where individual variables matter but don't explain everything alone. As a result, R^2 and p-values for coefficients do not inherently conflict. Since R^2 reflects predictive power, while p-values reveal significant relationships, highlighting different perspectives on the model's usefulness and limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68570ec3",
   "metadata": {},
   "source": [
    "5. The first cell prepares the data by splitting it into training and test data sets. Specifically, it divides the data into a 50-50 split, so that one part (pokeaman_train) is used to train the model, while the other part (pokeaman_test) is reserved for testing. This separation is essential to evaluate how well the model generalizes to new data. By keeping the training and testing data separate, we can determine whether the model is capturing true patterns in the data or simply memorizing the training data, which would lead to poor generalization.\n",
    "    In the second cell, we define a simple linear regression model that predicts HP using attack and defense as predictors. The model is trained on the pokeaman_train data to estimate the relationship between these predictors and the outcome HP. After fitting the model, we generate summary statistics, including the R^2 value, which indicates how well the model explains the variability in HP in the training data. This initial model fit gives us a sense of its performance on data it has seen before.\n",
    "    The third cell focuses on evaluating the model's performance using two metrics: \"in sample\" R^2 and \"out of sample\" R^2. The \"in sample\" R^2 measures how much of the variability in HP the model explains in the training data, while the \"out of sample\" R^2 evaluates how well the model predicts HP for the unseen test data (pokeaman_test). If there is a large drop in out-of-sample performance compared to in-sample performance, it indicates that the model is overfitting the training data and not generalizing well to new data.\n",
    "    The fourth cell specifies a more complex regression model that predicts HP using multiple predictors, including Attack, Defense, Speed, Legendary, Sp. Def, and Sp. Atk. This model includes interactions between these predictors to capture more detailed relationships in the data. While a complex model can increase in-sample R^2 by fitting the training data more closely, it runs the risk of overfitting. That is, the model may perform well on the training data but fail to generalize to new data.\n",
    "    The last cell compares the in-sample and out-of-sample R^2 values for the complex model. A high in-sample R^2 combined with a significantly lower out-of-sample R^2 indicates that the model may be overfitting, capturing noise and specific patterns in the training data that do not generalize. This illustrates the importance of balancing model complexity with the ability to generalize. Even if individual predictors appear significant in the training data, consistent performance on unseen data is required to confirm that the model is capturing meaningful, generalizable relationships.\n",
    "    \n",
    "link of chat log histories: https://chatgpt.com/share/6733f342-d2b4-8004-aad9-bd2ad05163c3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0841150a",
   "metadata": {},
   "source": [
    "6. In the logistic regression model, predictors such as age, province, and an extraversion score are used to predict a binary outcome. This initial formulation captures direct associations without introducing excessive complexity. Similarly, the additive linear regression model predicts the extraversion score using age, province, and an agreeableness score in a straightforward manner. Both approaches focus on capturing simpler, more interpretable relationships and avoid unnecessary model complexity that could lead to overfitting.\n",
    "    Introducing interaction terms increases model complexity by accounting for potential combined effects between predictors, such as the interaction between age and extraversion score. While this allows the model to capture more nuanced patterns in the data, it also increases the risk of overfitting. Overfitting occurs when a model learns specific, often noisy patterns from the training data that fail to generalize to new data. This is similar to the challenges faced in Model4, where the inclusion of many predictors and interactions led to poor generalizability.\n",
    "    Multicollinearity is a key issue when using multiple predictors and interaction terms. When predictors are highly correlated, it becomes difficult to accurately estimate their individual contributions to the outcome. For example, an interaction term such as DEMO_age * PSYCH_big_five_inventory_extraverted_score can introduce correlations that inflate the variance of the coefficient estimates. High multicollinearity makes model estimates sensitive to small changes in the data, reducing reliability and generalizability.\n",
    "    Models without interaction terms assume independent contributions of each predictor, leading to simpler and more interpretable results. This simplicity often reduces overfitting and improves generalizability. Models with interaction terms, however, allow flexibility by allowing predictor effects to vary based on other variables, providing a detailed but potentially overly complex view of relationships. Such complexity may capture spurious patterns if not well supported by the data.\n",
    "    Tools such as condition number or variance inflation factors (VIF) can be used to diagnose multicollinearity. A high condition number signals severe multicollinearity, indicating correlated predictors and reducing the stability of estimates. In logistic regression, examining correlations among continuous predictors can also reveal multicollinearity problems. Moreover, visualization helps to understand model behavior. For logistic regression, plotting probabilities against predictors illustrates how the model predicts binary outcomes. For linear regression, visualizing predicted values against predictors shows whether the added complexity of interactions provides meaningful insight or simply adds noise.\n",
    "\n",
    "link of chat log histories: https://chatgpt.com/share/6733f569-6354-8004-84bf-1fdcb803e216"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c79786b",
   "metadata": {},
   "source": [
    "7. Evolving from simpler additive models to more complex interaction-based models may increase complexity to better capture fine-grained relationships, or it may simplify the model to retain only the underlying predictors. Overall, the focus of model optimization is always on balancing complexity and simplicity to achieve generalizable and reliable predictions while addressing potential problems such as overfitting and multicollinearity.\n",
    "    Model5 extends previous models by incorporating a broader set of predictors, including numerical features such as Attack, Defense, Speed, Sp. Def, and Sp. Atk, along with an indicator variable Legendary and categorical variables Generation, Type 1, and Type 2. The goal of this expansion is to capture more potential relationships with the target variable HP. By including both continuous and categorical predictors, the model aims to identify more detailed patterns in the data. However, care is taken to avoid excessive complexity that could lead to overfitting, where the model is too well tuned to the training data and performs poorly on new data.\n",
    "    Model6 builds on the structure of Model5 by simplifying its linear form. This version focuses on a smaller set of continuous variables-specifically, Attack, Speed, Sp. Def, and Sp. Atk - while retaining important indicators from model5, such as type 1 indicators for \"Normal\" and \"Water\" and generation indicators for generations 2 and 5. This refinement aims to strike a balance between complexity and parsimony. By retaining only predictors that contribute meaningfully to the model, noise is reduced and problems such as overfitting and multicollinearity are mitigated, potentially improving prediction accuracy for both training and test datasets.\n",
    "    Model7 adds complexity by introducing interaction terms between the continuous predictors - Attack, Speed, Sp. Def, and Sp. Atk. This step allows the model to capture potential synergistic effects between these variables, revealing more complex patterns. The significant indicators from Model 6 are retained to maintain consistency in the model structure. In addition, centering and scaling are applied to continuous predictors to improve numerical stability and reduce multicollinearity. The goal is to increase predictive power while ensuring that the model remains stable and interpretable.\n",
    "\n",
    "link of chat log histories: https://chatgpt.com/share/6733f872-499c-8004-a8eb-9f6d0c56f7e9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdadf5aa",
   "metadata": {},
   "source": [
    "8. The purpose of this demonstration is to illustrate the variability in model performance across different random train-test splits of the data. By repeatedly splitting the data into training and test sets without using a fixed seed, this approach captures how consistent or variable the model's predictive power is, both for the data it was trained on (\"in-sample\") and for unseen data (\"out-of-sample\").\n",
    "    The results help to highlight several key points: 1) Model generalization: If a model performs consistently well on both the training and test sets across the different splits, it indicates good generalizability. This means that the model has captured underlying patterns that hold true when tested on new data. 2) Overfitting detection: A noticeable gap between in-sample and out-of-sample R-squared values across iterations often indicates overfitting. The model may be performing exceptionally well on the training data by capturing noise or specific quirks that do not generalize to new data. 3) Stability of performance: The scatter plot of in-sample versus out-of-sample R-squared values visualizes the variability in model performance. If the points cluster tightly around the diagonal line (y = x), it indicates that the model's performance is stable regardless of the data split. Wide variation indicates that the model's predictive power is highly dependent on the specific data used for training, indicating potential instability or sensitivity. 4) Importance of cross-validation: This demonstration highlights why relying on a single train-test split can be misleading. A model that appears to perform well on one split may not generalize as effectively when evaluated on a different split. Repeated evaluations help provide a more robust assessment of model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cc94a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assume 'songs' is your dataset\n",
    "linear_form = 'danceability ~ energy * loudness + energy * mode'\n",
    "\n",
    "# Define the number of repetitions\n",
    "reps = 100\n",
    "in_sample_Rsquared = np.empty(reps)\n",
    "out_of_sample_Rsquared = np.empty(reps)\n",
    "\n",
    "for i in range(reps):\n",
    "    # Split the data randomly each iteration without setting a seed\n",
    "    songs_training_data, songs_testing_data = train_test_split(songs, train_size=0.5)\n",
    "    \n",
    "    # Fit the model on the training set\n",
    "    final_model_fit = smf.ols(formula=linear_form, data=songs_training_data).fit()\n",
    "    \n",
    "    # In-sample R-squared (training data)\n",
    "    in_sample_Rsquared[i] = final_model_fit.rsquared\n",
    "    \n",
    "    # Out-of-sample R-squared (testing data)\n",
    "    predicted_test_values = final_model_fit.predict(songs_testing_data)\n",
    "    out_of_sample_Rsquared[i] = np.corrcoef(songs_testing_data['danceability'], predicted_test_values)[0, 1] ** 2\n",
    "\n",
    "# Create a DataFrame to store R-squared values\n",
    "df = pd.DataFrame({\n",
    "    \"In Sample Performance (R-squared)\": in_sample_Rsquared,\n",
    "    \"Out of Sample Performance (R-squared)\": out_of_sample_Rsquared\n",
    "})\n",
    "\n",
    "# Visualization using Plotly\n",
    "fig = px.scatter(df, x=\"In Sample Performance (R-squared)\", y=\"Out of Sample Performance (R-squared)\",\n",
    "                 title=\"In-sample vs Out-of-sample R-squared values over 100 random splits\",\n",
    "                 labels={\"In Sample Performance (R-squared)\": \"In-sample R-squared\",\n",
    "                         \"Out of Sample Performance (R-squared)\": \"Out-of-sample R-squared\"})\n",
    "\n",
    "# Add a y=x line for reference\n",
    "fig.add_trace(go.Scatter(x=[0, 1], y=[0, 1], mode=\"lines\", name=\"y=x\", line=dict(dash=\"dash\")))\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accd7d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "link of chat log histories: https://chatgpt.com/share/6733fcec-a31c-8004-b864-6fc2122acb29"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79abf6a8",
   "metadata": {},
   "source": [
    "9. The cells compare the generalizability and predictive performance of two models, model7 and model6, using different subsets of Pokémon data. Cell 1 evaluates Model7 by fitting it to data from Generation 1 Pokémon. The \"in-sample\" R-squared value measures how well it captures patterns within the training data, while the \"out-of-sample\" performance assesses its ability to predict the HP of Pokémon from generations not included in the training set, highlighting generalizability and potential overfitting risks. Cell 2 extends Model7's training data to include Generations 1 through 5, excluding Generation 6. In-sample performance is compared to predictions made on Generation 6 data to evaluate how well model7 adapts when trained on a broader subset. Cell 3 performs a similar evaluation with model6, fitting it to Generation 1 data and comparing in-sample and out-of-sample R-squared values to assess generalizability to other generations, providing a direct comparison with model7. Finally, Cell 4 trains Model6 on data from Generations 1 through 5 and tests its predictions on Generation 6 Pokémon, allowing a comparison of generalizability and consistency between the simpler Model6 and the more complex Model7. This setup provides insight into how model complexity affects predictive performance and generalizability across different datasets.\n",
    "    The purpose of the figure is to assess model generalizability using a more practical approach to data prediction. Rather than simply relying on random train-test splits, models are tested on data from different generations by training on earlier generations and making predictions on later generations. This sequential prediction approach reflects real-world scenarios where new data becomes available over time and models trained on historical data are used to predict future results. By evaluating in-sample and out-of-sample R-squared values, the results show that while model7_fit excels at capturing complex relationships within the training data, its ability to generalize to new, unseen data is limited, indicating potential overfitting issues. In contrast, model6_fit shows more stable performance across different datasets due to its simpler and more robust structure.\n",
    "    The key takeaway from this comparison is that there are risks associated with model complexity. Although the added complexity of model7_fit allows it to capture finer patterns, it also makes it more prone to overfitting, which reduces its reliability when applied to new data. On the other hand, model6_fit benefits from a simpler structure that provides better interpretability and ensures more consistent generalization. This is particularly valuable in real-world applications, where model reliability and ease of understanding often outweigh small gains in predictive accuracy. The sequential prediction approach also highlights the practical value of building models that adapt to future data based on past observations. While model7_fit may perform well in idealized environments, its limitations become more apparent when tested on unseen data.\n",
    "    \n",
    "link of chat log histories: https://chatgpt.com/share/67340d94-53d0-8004-8c37-3dd2a1bf2471"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
